{ "cells": [ { "cell_type": "markdown", "metadata": {}, "source": [ "# Classificação (adaptado)\n", "\n", "Notebook adaptado para: tratamento de desbalanceamento, métricas robustas, validação com GridSearchCV, calibração de probabilidades, tratamento básico de outliers e ajuste do pipeline polinomial." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "import warnings\n", "warnings.filterwarnings('ignore')\n", "\n", "import pandas as pd\n", "import numpy as np\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "\n", "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n", "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.naive_bayes import GaussianNB\n", "from sklearn.metrics import (\n", "    accuracy_score, roc_auc_score, classification_report,\n", "    confusion_matrix, precision_recall_curve, average_precision_score,\n", "    roc_curve\n", ")\n", "from sklearn.calibration import CalibratedClassifierCV\n", "\n", "# Opcional: imbalanced-learn para SMOTE\n", "# !pip install imbalanced-learn\n", "from imblearn.over_sampling import SMOTE\n", "from imblearn.pipeline import Pipeline as ImbPipeline\n", "\n", "pd.set_option('display.max_columns', None)\n", "plt.style.use('seaborn')\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Carregar o dataframe (ajuste o caminho se necessário)\n", "df = pd.read_parquet('df_final')\n", "df.shape\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "df.head()\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Verificar proporção de classes\n", "print('Distribuição da variável alvo:')\n", "print(df['SeriousDlqin2yrs'].value_counts())\n", "print('\nProporção de positivos (1):', df['SeriousDlqin2yrs'].mean())\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## Feature engineering / tratamento simples de outliers\n", "Transformações simples para reduzir skew e outliers extremos." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "df_proc = df.copy()\n", "\n", "# Tratar MonthlyIncome com log1p (evita problema com zero)\n", "df_proc['MonthlyIncome_log'] = np.log1p(df_proc['MonthlyIncome'].clip(lower=0))\n", "\n", "# Limitar DebtRatio e DebtPerLoan para reduzir outliers extremos (ex.: clip)\n", "df_proc['DebtRatio_clipped'] = df_proc['DebtRatio'].clip(upper=10)  # ajuste se necessario\n", "df_proc['DebtPerLoan_clipped'] = df_proc['DebtPerLoan'].clip(upper=100)  # ajuste se necessario\n", "\n", "# Remover colunas originais muito problemáticas (mantemos as transformadas)\n", "cols_to_use = [\n", "    'RevolvingUtilizationOfUnsecuredLines', 'age', 'NumberOfTime30-59DaysPastDueNotWorse',\n", "    'NumberOfOpenCreditLinesAndLoans', 'NumberOfTimes90DaysLate', 'NumberRealEstateLoansOrLines',\n", "    'NumberOfTime60-89DaysPastDueNotWorse', 'NumberOfDependents', 'IncomePerDependent',\n", "    'Pagamentos_atrasados_Total', 'MonthlyIncome_log', 'DebtRatio_clipped', 'DebtPerLoan_clipped'\n", "]\n", "\n", "X = df_proc[cols_to_use]\n", "y = df_proc['SeriousDlqin2yrs']\n", "\n", "X.shape\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Divisão treino/teste com stratify\n", "X_train, X_test, y_train, y_test = train_test_split(\n", "    X, y, test_size=0.3, random_state=42, stratify=y\n", ")\n", "print('Train:', X_train.shape, 'Test:', X_test.shape)\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## Definir pipelines\n", "Construí 3 pipelines:\n", "- Regressão logística (linear)\n", "- Naive Bayes (Gaussian)\n", "- Regressão logística com interações polinomiais (degree=2) — PolynomialFeatures antes do scaler\n", "\n", "Incluí também exemplo com SMOTE (opcional) usando imblearn Pipeline — lembre-se: SMOTE deve ser aplicado apenas no treino." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "pipe_logreg = Pipeline([\n", "    ('scaler', StandardScaler()),\n", "    ('model', LogisticRegression(max_iter=5000, solver='saga', class_weight='balanced', random_state=42))\n", "])\n", "\n", "pipe_nb = Pipeline([\n", "    ('scaler', StandardScaler()),\n", "    ('model', GaussianNB())\n", "])\n", "\n", "# Polinomial: gerar polinômios primeiro, depois escalar os atributos expandidos\n", "pipe_poly = Pipeline([\n", "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n", "    ('scaler', StandardScaler()),\n", "    ('model', LogisticRegression(max_iter=5000, solver='saga', class_weight='balanced', random_state=42))\n", "])\n", "\n", "# Exemplo opcional com SMOTE (usar ImbPipeline para garantir que SMOTE ocorra dentro do CV corretamente se desejado)\n", "pipe_logreg_smote = ImbPipeline([\n", "    ('smote', SMOTE(random_state=42)),\n", "    ('scaler', StandardScaler()),\n", "    ('model', LogisticRegression(max_iter=5000, solver='saga', random_state=42))\n", "])\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## GridSearchCV para otimizar regularização (C) e, se quiser, degree do polinômio\n", "Usamos StratifiedKFold para evitar vazamento de proporções de classe." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n", "\n", "param_logreg = {\n", "    'model__C': [0.01, 0.1, 1, 10]\n", "}\n", "param_poly = {\n", "    'poly__degree': [2],  # ja definido; se quiser testar, colocar [2,3]\n", "    'model__C': [0.01, 0.1, 1, 10]\n", "}\n", "\n", "gscv_logreg = GridSearchCV(pipe_logreg, param_logreg, scoring='roc_auc', cv=cv, n_jobs=-1)\n", "gscv_nb = GridSearchCV(pipe_nb, {'model__var_smoothing': [1e-9, 1e-8, 1e-7]}, scoring='roc_auc', cv=cv, n_jobs=-1)\n", "gscv_poly = GridSearchCV(pipe_poly, param_poly, scoring='roc_auc', cv=cv, n_jobs=-1)\n", "\n", "models_cv = {\n", "    'Logística': gscv_logreg,\n", "    'NaiveBayes': gscv_nb,\n", "    'LogísticaPolinomial': gscv_poly\n", "}\n", "\n", "resultados = []\n", "\n", "for nome, gscv in models_cv.items():\n", "    print(f'Fazendo CV e GridSearch para: {nome}')\n", "    gscv.fit(X_train, y_train)\n", "    print('Melhor params:', gscv.best_params_, 'Best CV ROC-AUC:', gscv.best_score_)\n", "    resultados.append({'Modelo': nome, 'BestParams': gscv.best_params_, 'CV_ROC_AUC': gscv.best_score_})\n", "\n", "resultados_df = pd.DataFrame(resultados)\n", "display(resultados_df)\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## Avaliação final no conjunto de teste\n", "Usamos o melhor estimador encontrado pelo GridSearch para cada modelo, calculamos várias métricas e plotamos ROC e Precision-Recall." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "evals = []\n", "\n", "plt.figure(figsize=(12,5))\n", "plt.subplot(1,2,1)\n", "for nome, gscv in models_cv.items():\n", "    best = gscv.best_estimator_\n", "    # Calibrar probabilidades para modelos que não as emitem bem (opcional)\n", "    clf = CalibratedClassifierCV(best, cv=3) if nome!='NaiveBayes' else best\n", "    try:\n", "        clf.fit(X_train, y_train)\n", "    except Exception:\n", "        # se o best ja estiver calibrado ou ocorrer erro, usa o best diretamente\n", "        clf = best\n", "\n", "    # Previsoes e probabilidades\n", "    y_pred = clf.predict(X_test)\n", "    y_prob = clf.predict_proba(X_test)[:,1] if hasattr(clf, 'predict_proba') else None\n", "\n", "    acc = accuracy_score(y_test, y_pred)\n", "    auc = roc_auc_score(y_test, y_prob) if y_prob is not None else np.nan\n", "    ap = average_precision_score(y_test, y_prob) if y_prob is not None else np.nan\n", "    report = classification_report(y_test, y_pred, output_dict=True)\n", "\n", "    evals.append({\n", "        'Modelo': nome,\n", "        'Acurácia': acc,\n", "        'ROC_AUC': auc,\n", "        'PR_AUC (AP)': ap,\n", "        'Precision': report['1']['precision'] if '1' in report else None,\n", "        'Recall': report['1']['recall'] if '1' in report else None,\n", "        'F1': report['1']['f1-score'] if '1' in report else None\n", "    })\n", "\n", "    # ROC curve\n", "    if y_prob is not None:\n", "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n", "        plt.plot(fpr, tpr, label=f'{nome} (AUC={auc:.3f})')\n", "\n", "plt.plot([0,1],[0,1],'k--')\n", "plt.xlabel('False Positive Rate')\n", "plt.ylabel('True Positive Rate')\n", "plt.title('ROC Curves')\n", "plt.legend()\n", "\n", "plt.subplot(1,2,2)\n", "for nome, gscv in models_cv.items():\n", "    best = gscv.best_estimator_\n", "    clf = CalibratedClassifierCV(best, cv=3) if nome!='NaiveBayes' else best\n", "    try:\n", "        clf.fit(X_train, y_train)\n", "    except Exception:\n", "        clf = best\n", "    y_prob = clf.predict_proba(X_test)[:,1] if hasattr(clf, 'predict_proba') else None\n", "    if y_prob is not None:\n", "        prec, rec, _ = precision_recall_curve(y_test, y_prob)\n", "        ap = average_precision_score(y_test, y_prob)\n", "        plt.plot(rec, prec, label=f'{nome} (AP={ap:.3f})')\n", "\n", "plt.xlabel('Recall')\n", "plt.ylabel('Precision')\n", "plt.title('Precision-Recall Curves')\n", "plt.legend()\n", "plt.tight_layout()\n", "plt.show()\n", "\n", "evals_df = pd.DataFrame(evals)\n", "display(evals_df)\n" ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Mostrar matrizes de confusão e classification_report detalhado para o melhor modelo (exemplo)\n", "best_name = evals_df.sort_values('ROC_AUC', ascending=False).iloc[0]['Modelo']\n", "best_gscv = models_cv[best_name]\n", "best_est = best_gscv.best_estimator_\n", "clf = CalibratedClassifierCV(best_est, cv=3) if best_name!='NaiveBayes' else best_est\n", "try:\n", "    clf.fit(X_train, y_train)\n", "except Exception:\n", "    clf = best_est\n", "\n", "y_pred = clf.predict(X_test)\n", "y_prob = clf.predict_proba(X_test)[:,1] if hasattr(clf, 'predict_proba') else None\n", "\n", "print('Melhor modelo:', best_name)\n", "print('\nClassification report:')\n", "print(classification_report(y_test, y_pred))\n", "\n", "cm = confusion_matrix(y_test, y_pred)\n", "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n", "plt.xlabel('Previsto')\n", "plt.ylabel('Real')\n", "plt.title(f'Matriz de Confusão - {best_name}')\n", "plt.show()\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## Observações finais e próximos passos\n", "- Se o dataset for muito desbalanceado (ex.: < 5% positivos), prefira otimizar por PR-AUC (average_precision) e olhar recall/precision no limiar escolhido.\n", "- Teste SMOTE (ou undersampling / combinação) dentro de um pipeline com CV para evitar leak. Use ImbPipeline quando incluir SMOTE.\n", "- Experimente regularização (C), penalidades L1/L2 e seleção de features (feature selection) para reduzir overfitting, especialmente com PolynomialFeatures.\n", "- Verifique possíveis vazamentos ao criar features (garanta que foram feitas apenas com informações disponíveis no momento da previsão).\n", "- Calibração é importante se você precisa de probabilidades bem calibradas para decisões (ex.: score de crédito com thresholds comerciais).\n", "\n", "Se quiser, eu adapto o notebook ainda mais: implemento GridSearch/RandomizedSearch incluindo SMOTE dentro do CV (ImbPipeline), ou eu submeto este notebook de volta ao repositório em uma nova branch com commit. Qual opção prefere?" ] }], "metadata": { "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" }, "language_info": { "codemirror_mode": { "name": "ipython", "version": 3 }, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.13.7" } }, "nbformat": 4, "nbformat_minor": 5 }